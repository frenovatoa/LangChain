{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar las librerías contenidas en el archivo requirements. La bandera -q es para que no muestre los mensajes de instalación.\n",
    "# ! Sirve para indicar al notebook que el comando se ejecuta en el shell y no en Python.\n",
    "!pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.302\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\Admin\\anaconda3\\envs\\LangChain\\Lib\\site-packages\n",
      "Requires: aiohttp, anyio, dataclasses-json, jsonpatch, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Ver la librería de LangChain instalada.\n",
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si la librería ya está instalada, actualizarla de ser necesario.\n",
    "!pip install langchain --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurar dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías os (para acceder a variables de entorno) y dotenv (para cargar variables de entorno desde un archivo .env).\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar variables de entorno desde el archivo .env.\n",
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validar que el archivo .env se haya cargado correctamente obteniendo el valor de la variable de entorno OPENAI_API_KEY.\n",
    "os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Otra forma de cargar el archivo .env es con la función find_dotenv() (es más recomendable).\n",
    "# La propiedad override=True permite sobreescribir las variables de entorno que ya existen.\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validar que el archivo .env se haya cargado correctamente obteniendo el valor de la variable de entorno OPENAI_API_KEY.\n",
    "os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Models (GPT 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El modelo text-davinci-003 es suficiente para generar textos de calidad.\n",
    "# La temperatura, mientras más alta, más creativo será el texto generado (0-2).\n",
    "# max_tokens representa el número máximo de tokens a generar.\n",
    "gpt3 = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'max_tokens': 1024, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\n"
     ]
    }
   ],
   "source": [
    "print(gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Una base de datos es una colección organizada de datos almacenados en una computadora y accesibles para su uso por programas de aplicaciones. Los datos almacenados en una base de datos se relacionan entre sí y están estructurados en un formato específico que permite a los usuarios recuperar y procesar información de manera eficiente. Las bases de datos se utilizan para almacenar y administrar grandes cantidades de información que de otra manera sería difícil de manejar.\n"
     ]
    }
   ],
   "source": [
    "respuesta = gpt3(\"Explica qué es una base de datos.\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "# Obtener el número de tokens de la respuesta generada. Se consideran los tokens de entrada y salida.\n",
    "# El costo de la API es de 4 tokens por cada 1,000 tokens generados.\n",
    "print(gpt3.get_num_tokens(\"Explica qué es una base de datos.\"))\n",
    "print(gpt3.get_num_tokens(respuesta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer múltiples solicitudes al modelo.\n",
    "respuestas = gpt3.generate([\"El luchador mexicano más popular fue: \", \n",
    "                           \"Lista 5 palabras que definan la filosofía: \",\n",
    "                           \"Explica brevemente qué es big data: \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\nEl luchador mexicano más popular es sin duda el \"Rey\" Místico, también conocido como El Solitario. Es un luchador profesional mexicano que ha ganado numerosos títulos en toda su carrera. Ha participado en varias promociones profesionales de lucha libre, incluyendo la Consejo Mundial de Lucha Libre (CMLL) y la Asistencia Asesoría y Administración (AAA). Es uno de los luchadores más queridos en México y se le considera un ícono de la lucha libre profesional mexicana.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n1. Ética \\n2. Razón \\n3. Existencia \\n4. Verdad \\n5. Pensamiento', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nBig Data es un término que se refiere a los conjuntos de datos masivos, estructurados o no estructurados, que son demasiado grandes o complejos para ser procesados y analizados con herramientas tradicionales de procesamiento de datos. Esta información se puede usar para identificar patrones, tendencias y correlaciones entre los datos, lo que permite a las empresas mejorar sus productos, servicios y estrategias de marketing.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 381, 'prompt_tokens': 43, 'completion_tokens': 338}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('71ac0f9b-df36-4fd3-8817-81c4e4566853')), RunInfo(run_id=UUID('900e4694-3d09-498d-9723-60d77687b591')), RunInfo(run_id=UUID('bac48d50-fb8e-4067-a69e-71f2a8cdb09a'))]\n"
     ]
    }
   ],
   "source": [
    "print(respuestas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='\\n\\nEl luchador mexicano más popular es sin duda el \"Rey\" Místico, también conocido como El Solitario. Es un luchador profesional mexicano que ha ganado numerosos títulos en toda su carrera. Ha participado en varias promociones profesionales de lucha libre, incluyendo la Consejo Mundial de Lucha Libre (CMLL) y la Asistencia Asesoría y Administración (AAA). Es uno de los luchadores más queridos en México y se le considera un ícono de la lucha libre profesional mexicana.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n1. Ética \\n2. Razón \\n3. Existencia \\n4. Verdad \\n5. Pensamiento', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nBig Data es un término que se refiere a los conjuntos de datos masivos, estructurados o no estructurados, que son demasiado grandes o complejos para ser procesados y analizados con herramientas tradicionales de procesamiento de datos. Esta información se puede usar para identificar patrones, tendencias y correlaciones entre los datos, lo que permite a las empresas mejorar sus productos, servicios y estrategias de marketing.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
     ]
    }
   ],
   "source": [
    "# Hacer más legibles las múltiples respuestas generadas.\n",
    "print(respuestas.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation(text='\\n\\n1. Ética \\n2. Razón \\n3. Existencia \\n4. Verdad \\n5. Pensamiento', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
     ]
    }
   ],
   "source": [
    "# Obtener el segundo elemento de la lista de respuestas generadas.\n",
    "print(respuestas.generations[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='\\n\\n1. Ética \\n2. Razón \\n3. Existencia \\n4. Verdad \\n5. Pensamiento' generation_info={'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "# Obtener el primer elemento de la lista anterior. Cada vez se está más cerca del resultado final.\n",
    "print(respuestas.generations[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Ética \n",
      "2. Razón \n",
      "3. Existencia \n",
      "4. Verdad \n",
      "5. Pensamiento\n"
     ]
    }
   ],
   "source": [
    "# La respuesta anterior es un diccionario. Obtener el valor de la llave text.\n",
    "print(respuestas.generations[1][0].text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
